name: Performance Monitoring

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  bundle-size-analysis:
    name: Bundle Size Analysis
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Build application
        run: npm run build:web
        
      - name: Analyze bundle size
        run: |
          # Install bundle analyzer
          npm install --no-save webpack-bundle-analyzer bundlesize
          
          # Generate bundle analysis
          npx webpack-bundle-analyzer dist/static/js/*.js --report --mode static --report-filename bundle-report.html --no-open
          
          # Calculate bundle sizes
          TOTAL_SIZE=$(du -sb dist | cut -f1)
          JS_SIZE=$(find dist -name "*.js" -exec du -cb {} + | tail -1 | cut -f1)
          CSS_SIZE=$(find dist -name "*.css" -exec du -cb {} + | tail -1 | cut -f1)
          
          echo "üìä Bundle Size Analysis:"
          echo "- Total: $(($TOTAL_SIZE / 1024))KB"
          echo "- JavaScript: $(($JS_SIZE / 1024))KB"
          echo "- CSS: $(($CSS_SIZE / 1024))KB"
          
          # Store sizes for comparison
          echo "total_size=$TOTAL_SIZE" >> $GITHUB_ENV
          echo "js_size=$JS_SIZE" >> $GITHUB_ENV
          echo "css_size=$CSS_SIZE" >> $GITHUB_ENV
          
      - name: Compare with main branch
        if: github.event_name == 'pull_request'
        run: |
          # Switch to main branch
          git fetch origin main
          git checkout origin/main
          
          # Build main branch
          npm ci
          npm run build:web
          
          # Calculate main branch sizes
          MAIN_TOTAL_SIZE=$(du -sb dist | cut -f1)
          MAIN_JS_SIZE=$(find dist -name "*.js" -exec du -cb {} + | tail -1 | cut -f1)
          MAIN_CSS_SIZE=$(find dist -name "*.css" -exec du -cb {} + | tail -1 | cut -f1)
          
          # Calculate differences
          TOTAL_DIFF=$((${{ env.total_size }} - $MAIN_TOTAL_SIZE))
          JS_DIFF=$((${{ env.js_size }} - $MAIN_JS_SIZE))
          CSS_DIFF=$((${{ env.css_size }} - $MAIN_CSS_SIZE))
          
          echo "üìà Size Comparison with main:"
          echo "- Total: $(($TOTAL_DIFF / 1024))KB change"
          echo "- JavaScript: $(($JS_DIFF / 1024))KB change"
          echo "- CSS: $(($CSS_DIFF / 1024))KB change"
          
          # Fail if bundle size increased significantly
          if [ $TOTAL_DIFF -gt 512000 ]; then  # 500KB
            echo "‚ùå Bundle size increased by more than 500KB!"
            exit 1
          fi
          
      - name: Upload bundle analysis
        uses: actions/upload-artifact@v4
        with:
          name: bundle-analysis-${{ github.sha }}
          path: bundle-report.html
          retention-days: 30

  lighthouse-audit:
    name: Lighthouse Performance Audit
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Build application
        run: npm run build:web
        
      - name: Install Lighthouse
        run: npm install --no-save lighthouse
        
      - name: Start web server
        run: |
          npx http-server dist -p 8080 &
          echo $! > server.pid
          sleep 5
          
      - name: Run Lighthouse audit
        run: |
          npx lighthouse http://localhost:8080 \
            --output=json \
            --output=html \
            --output-path=lighthouse-report \
            --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage" \
            --quiet
            
      - name: Parse Lighthouse results
        run: |
          # Extract key metrics
          PERFORMANCE=$(cat lighthouse-report.json | jq '.categories.performance.score * 100')
          ACCESSIBILITY=$(cat lighthouse-report.json | jq '.categories.accessibility.score * 100')
          BEST_PRACTICES=$(cat lighthouse-report.json | jq '.categories["best-practices"].score * 100')
          SEO=$(cat lighthouse-report.json | jq '.categories.seo.score * 100')
          
          FCP=$(cat lighthouse-report.json | jq '.audits["first-contentful-paint"].numericValue')
          LCP=$(cat lighthouse-report.json | jq '.audits["largest-contentful-paint"].numericValue')
          CLS=$(cat lighthouse-report.json | jq '.audits["cumulative-layout-shift"].numericValue')
          
          echo "üöÄ Lighthouse Scores:"
          echo "- Performance: ${PERFORMANCE}%"
          echo "- Accessibility: ${ACCESSIBILITY}%"
          echo "- Best Practices: ${BEST_PRACTICES}%"
          echo "- SEO: ${SEO}%"
          echo ""
          echo "üìä Core Web Vitals:"
          echo "- First Contentful Paint: ${FCP}ms"
          echo "- Largest Contentful Paint: ${LCP}ms"
          echo "- Cumulative Layout Shift: ${CLS}"
          
          # Store results
          echo "performance_score=$PERFORMANCE" >> $GITHUB_ENV
          echo "accessibility_score=$ACCESSIBILITY" >> $GITHUB_ENV
          echo "fcp=$FCP" >> $GITHUB_ENV
          echo "lcp=$LCP" >> $GITHUB_ENV
          echo "cls=$CLS" >> $GITHUB_ENV
          
      - name: Check performance thresholds
        run: |
          # Define thresholds
          MIN_PERFORMANCE=80
          MIN_ACCESSIBILITY=90
          MAX_FCP=2000
          MAX_LCP=4000
          MAX_CLS=0.1
          
          FAILED=false
          
          # Check performance score
          if (( $(echo "${{ env.performance_score }} < $MIN_PERFORMANCE" | bc -l) )); then
            echo "‚ùå Performance score (${{ env.performance_score }}%) below threshold ($MIN_PERFORMANCE%)"
            FAILED=true
          fi
          
          # Check accessibility score
          if (( $(echo "${{ env.accessibility_score }} < $MIN_ACCESSIBILITY" | bc -l) )); then
            echo "‚ùå Accessibility score (${{ env.accessibility_score }}%) below threshold ($MIN_ACCESSIBILITY%)"
            FAILED=true
          fi
          
          # Check FCP
          if (( $(echo "${{ env.fcp }} > $MAX_FCP" | bc -l) )); then
            echo "‚ùå First Contentful Paint (${{ env.fcp }}ms) above threshold (${MAX_FCP}ms)"
            FAILED=true
          fi
          
          # Check LCP
          if (( $(echo "${{ env.lcp }} > $MAX_LCP" | bc -l) )); then
            echo "‚ùå Largest Contentful Paint (${{ env.lcp }}ms) above threshold (${MAX_LCP}ms)"
            FAILED=true
          fi
          
          # Check CLS
          if (( $(echo "${{ env.cls }} > $MAX_CLS" | bc -l) )); then
            echo "‚ùå Cumulative Layout Shift (${{ env.cls }}) above threshold ($MAX_CLS)"
            FAILED=true
          fi
          
          if [ "$FAILED" = true ]; then
            echo "Performance audit failed!"
            exit 1
          else
            echo "‚úÖ All performance metrics passed!"
          fi
          
      - name: Stop web server
        run: |
          if [ -f server.pid ]; then
            kill $(cat server.pid) || true
          fi
          
      - name: Upload Lighthouse reports
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-report-${{ github.sha }}
          path: |
            lighthouse-report.html
            lighthouse-report.json
          retention-days: 30

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Build application
        run: npm run build:web
        
      - name: Install Artillery
        run: npm install --no-save artillery
        
      - name: Create load test configuration
        run: |
          cat > load-test.yml << EOF
          config:
            target: 'http://localhost:8080'
            phases:
              - duration: 60
                arrivalRate: 10
              - duration: 120
                arrivalRate: 20
              - duration: 60
                arrivalRate: 5
            processor: './load-test-processor.js'
          scenarios:
            - name: 'Browse application'
              weight: 70
              flow:
                - get:
                    url: '/'
                - think: 2
                - get:
                    url: '/static/js/{{ \$randomString() }}.js'
                - think: 1
            - name: 'API calls'
              weight: 30
              flow:
                - post:
                    url: '/api/episodes'
                    json:
                      title: 'Test Episode'
                - think: 1
          EOF
          
      - name: Create load test processor
        run: |
          cat > load-test-processor.js << 'EOF'
          module.exports = {
            randomString: function() {
              return Math.random().toString(36).substring(7);
            }
          };
          EOF
          
      - name: Start web server
        run: |
          npx http-server dist -p 8080 &
          echo $! > server.pid
          sleep 5
          
      - name: Run load test
        run: |
          npx artillery run load-test.yml --output load-test-results.json
          
      - name: Generate load test report
        run: |
          npx artillery report load-test-results.json --output load-test-report.html
          
      - name: Analyze results
        run: |
          # Extract key metrics from results
          REQUESTS_COMPLETED=$(cat load-test-results.json | jq '.aggregate.counters["http.requests"]')
          RESPONSE_TIME_P95=$(cat load-test-results.json | jq '.aggregate.summaries["http.response_time"].p95')
          ERROR_RATE=$(cat load-test-results.json | jq '.aggregate.counters["http.codes.500"] // 0')
          
          echo "üìà Load Test Results:"
          echo "- Total Requests: $REQUESTS_COMPLETED"
          echo "- 95th Percentile Response Time: ${RESPONSE_TIME_P95}ms"
          echo "- Error Rate: $ERROR_RATE errors"
          
          # Check thresholds
          if (( $(echo "$RESPONSE_TIME_P95 > 2000" | bc -l) )); then
            echo "‚ùå Response time too high: ${RESPONSE_TIME_P95}ms"
            exit 1
          fi
          
          if [ "$ERROR_RATE" -gt "10" ]; then
            echo "‚ùå Too many errors: $ERROR_RATE"
            exit 1
          fi
          
          echo "‚úÖ Load test passed!"
          
      - name: Stop web server
        run: |
          if [ -f server.pid ]; then
            kill $(cat server.pid) || true
          fi
          
      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results-${{ github.sha }}
          path: |
            load-test-report.html
            load-test-results.json
          retention-days: 30

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Run memory profiling tests
        run: |
          # Create a simple memory test
          cat > memory-test.js << 'EOF'
          const { performance, PerformanceObserver } = require('perf_hooks');
          
          // Monitor memory usage
          const obs = new PerformanceObserver((list) => {
            const entries = list.getEntries();
            entries.forEach((entry) => {
              console.log(`${entry.name}: ${entry.duration}ms`);
            });
          });
          obs.observe({ entryTypes: ['measure'] });
          
          // Simulate app operations
          performance.mark('start');
          
          // Simulate memory-intensive operations
          const data = [];
          for (let i = 0; i < 100000; i++) {
            data.push({ id: i, data: 'test'.repeat(100) });
          }
          
          performance.mark('end');
          performance.measure('Memory Test', 'start', 'end');
          
          // Check memory usage
          const memUsage = process.memoryUsage();
          console.log('Memory Usage:');
          console.log(`- RSS: ${Math.round(memUsage.rss / 1024 / 1024)}MB`);
          console.log(`- Heap Used: ${Math.round(memUsage.heapUsed / 1024 / 1024)}MB`);
          console.log(`- Heap Total: ${Math.round(memUsage.heapTotal / 1024 / 1024)}MB`);
          console.log(`- External: ${Math.round(memUsage.external / 1024 / 1024)}MB`);
          
          // Check for memory leaks (simplified)
          if (memUsage.heapUsed > 100 * 1024 * 1024) { // 100MB
            console.log('‚ùå Potential memory leak detected!');
            process.exit(1);
          } else {
            console.log('‚úÖ Memory usage within acceptable limits');
          }
          EOF
          
          node memory-test.js

  performance-regression:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: [bundle-size-analysis, lighthouse-audit]
    
    steps:
      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: lighthouse-report-${{ github.sha }}
          path: current/
          
      - name: Check for performance regression
        run: |
          # This would typically compare against stored baseline metrics
          # For now, we'll just validate the current results exist
          if [ -f "current/lighthouse-report.json" ]; then
            CURRENT_PERFORMANCE=$(cat current/lighthouse-report.json | jq '.categories.performance.score * 100')
            echo "Current performance score: ${CURRENT_PERFORMANCE}%"
            
            # In a real scenario, you'd compare against a baseline
            # BASELINE_PERFORMANCE=85
            # if (( $(echo "$CURRENT_PERFORMANCE < $BASELINE_PERFORMANCE - 5" | bc -l) )); then
            #   echo "‚ùå Performance regression detected!"
            #   exit 1
            # fi
            
            echo "‚úÖ Performance check completed"
          else
            echo "‚ùå No performance data found"
            exit 1
          fi

  notify-performance:
    name: Notify Performance Results
    runs-on: ubuntu-latest
    needs: [bundle-size-analysis, lighthouse-audit, memory-profiling]
    if: always()
    
    steps:
      - name: Performance summary
        run: |
          echo "üéØ Performance Monitoring Summary:"
          echo "- Bundle Analysis: ${{ needs.bundle-size-analysis.result }}"
          echo "- Lighthouse Audit: ${{ needs.lighthouse-audit.result }}"
          echo "- Memory Profiling: ${{ needs.memory-profiling.result }}"
          
          if [[ "${{ needs.bundle-size-analysis.result }}" == "failure" || "${{ needs.lighthouse-audit.result }}" == "failure" || "${{ needs.memory-profiling.result }}" == "failure" ]]; then
            echo "‚ùå Performance issues detected!"
            exit 1
          else
            echo "‚úÖ All performance checks passed!"
          fi